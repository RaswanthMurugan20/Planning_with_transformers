{"format": "torch", "nodes": [{"name": "embed_timestep", "id": 140474691773888, "class_name": "Embedding(4096, 8)", "parameters": [["weight", [4096, 8]]], "output_shape": [[64, 100, 8]], "num_parameters": [32768]}, {"name": "embed_state", "id": 140474691773984, "class_name": "Linear(in_features=2, out_features=8, bias=True)", "parameters": [["weight", [8, 2]], ["bias", [8]]], "output_shape": [[64, 100, 8]], "num_parameters": [16, 8]}, {"name": "embed_action", "id": 140474691774032, "class_name": "Linear(in_features=1, out_features=8, bias=True)", "parameters": [["weight", [8, 1]], ["bias", [8]]], "output_shape": [[64, 100, 8]], "num_parameters": [8, 8]}, {"name": "embed_rtg", "id": 140474691773936, "class_name": "Linear(in_features=1, out_features=8, bias=True)", "parameters": [["weight", [8, 1]], ["bias", [8]]], "output_shape": [[64, 100, 8]], "num_parameters": [8, 8]}, {"name": "embed_ln", "id": 140474691773840, "class_name": "LayerNorm((8,), eps=1e-05, elementwise_affine=True)", "parameters": [["weight", [8]], ["bias", [8]]], "output_shape": [[64, 300, 8]], "num_parameters": [8, 8]}, {"name": "transformer.0", "id": 140474691771680, "class_name": "Block(\n  (attention): MaskedCausalAttention(\n    (q_net): Linear(in_features=8, out_features=8, bias=True)\n    (k_net): Linear(in_features=8, out_features=8, bias=True)\n    (v_net): Linear(in_features=8, out_features=8, bias=True)\n    (proj_net): Linear(in_features=8, out_features=8, bias=True)\n    (att_drop): Dropout(p=0.1, inplace=False)\n    (proj_drop): Dropout(p=0.1, inplace=False)\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=8, out_features=32, bias=True)\n    (1): GELU(approximate=none)\n    (2): Linear(in_features=32, out_features=8, bias=True)\n    (3): Dropout(p=0.1, inplace=False)\n  )\n  (ln1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n  (ln2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n)", "parameters": [["attention.q_net.weight", [8, 8]], ["attention.q_net.bias", [8]], ["attention.k_net.weight", [8, 8]], ["attention.k_net.bias", [8]], ["attention.v_net.weight", [8, 8]], ["attention.v_net.bias", [8]], ["attention.proj_net.weight", [8, 8]], ["attention.proj_net.bias", [8]], ["mlp.0.weight", [32, 8]], ["mlp.0.bias", [32]], ["mlp.2.weight", [8, 32]], ["mlp.2.bias", [8]], ["ln1.weight", [8]], ["ln1.bias", [8]], ["ln2.weight", [8]], ["ln2.bias", [8]]], "output_shape": [[64, 300, 8]], "num_parameters": [64, 8, 64, 8, 64, 8, 64, 8, 256, 32, 256, 8, 8, 8, 8, 8]}, {"name": "transformer.1", "id": 140474691772256, "class_name": "Block(\n  (attention): MaskedCausalAttention(\n    (q_net): Linear(in_features=8, out_features=8, bias=True)\n    (k_net): Linear(in_features=8, out_features=8, bias=True)\n    (v_net): Linear(in_features=8, out_features=8, bias=True)\n    (proj_net): Linear(in_features=8, out_features=8, bias=True)\n    (att_drop): Dropout(p=0.1, inplace=False)\n    (proj_drop): Dropout(p=0.1, inplace=False)\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=8, out_features=32, bias=True)\n    (1): GELU(approximate=none)\n    (2): Linear(in_features=32, out_features=8, bias=True)\n    (3): Dropout(p=0.1, inplace=False)\n  )\n  (ln1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n  (ln2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n)", "parameters": [["attention.q_net.weight", [8, 8]], ["attention.q_net.bias", [8]], ["attention.k_net.weight", [8, 8]], ["attention.k_net.bias", [8]], ["attention.v_net.weight", [8, 8]], ["attention.v_net.bias", [8]], ["attention.proj_net.weight", [8, 8]], ["attention.proj_net.bias", [8]], ["mlp.0.weight", [32, 8]], ["mlp.0.bias", [32]], ["mlp.2.weight", [8, 32]], ["mlp.2.bias", [8]], ["ln1.weight", [8]], ["ln1.bias", [8]], ["ln2.weight", [8]], ["ln2.bias", [8]]], "output_shape": [[64, 300, 8]], "num_parameters": [64, 8, 64, 8, 64, 8, 64, 8, 256, 32, 256, 8, 8, 8, 8, 8]}, {"name": "transformer.2", "id": 140474691773264, "class_name": "Block(\n  (attention): MaskedCausalAttention(\n    (q_net): Linear(in_features=8, out_features=8, bias=True)\n    (k_net): Linear(in_features=8, out_features=8, bias=True)\n    (v_net): Linear(in_features=8, out_features=8, bias=True)\n    (proj_net): Linear(in_features=8, out_features=8, bias=True)\n    (att_drop): Dropout(p=0.1, inplace=False)\n    (proj_drop): Dropout(p=0.1, inplace=False)\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=8, out_features=32, bias=True)\n    (1): GELU(approximate=none)\n    (2): Linear(in_features=32, out_features=8, bias=True)\n    (3): Dropout(p=0.1, inplace=False)\n  )\n  (ln1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n  (ln2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n)", "parameters": [["attention.q_net.weight", [8, 8]], ["attention.q_net.bias", [8]], ["attention.k_net.weight", [8, 8]], ["attention.k_net.bias", [8]], ["attention.v_net.weight", [8, 8]], ["attention.v_net.bias", [8]], ["attention.proj_net.weight", [8, 8]], ["attention.proj_net.bias", [8]], ["mlp.0.weight", [32, 8]], ["mlp.0.bias", [32]], ["mlp.2.weight", [8, 32]], ["mlp.2.bias", [8]], ["ln1.weight", [8]], ["ln1.bias", [8]], ["ln2.weight", [8]], ["ln2.bias", [8]]], "output_shape": [[64, 300, 8]], "num_parameters": [64, 8, 64, 8, 64, 8, 64, 8, 256, 32, 256, 8, 8, 8, 8, 8]}, {"name": "predict_rtg", "id": 140474691774080, "class_name": "Linear(in_features=8, out_features=1, bias=True)", "parameters": [["weight", [1, 8]], ["bias", [1]]], "output_shape": [[64, 100, 1]], "num_parameters": [8, 1]}, {"name": "predict_state", "id": 140474691774128, "class_name": "Linear(in_features=8, out_features=2, bias=True)", "parameters": [["weight", [2, 8]], ["bias", [2]]], "output_shape": [[64, 100, 2]], "num_parameters": [16, 2]}, {"name": "predict_action.0", "id": 140474691774176, "class_name": "Linear(in_features=8, out_features=1, bias=True)", "parameters": [["weight", [1, 8]], ["bias", [1]]], "output_shape": [[64, 100, 1]], "num_parameters": [8, 1]}, {"name": "predict_action.1", "id": 140474691774224, "class_name": "Tanh()", "parameters": [], "output_shape": [[64, 100, 1]], "num_parameters": []}], "edges": []}