============================================================
start time: 08-24-13-54
============================================================
device set to: cuda
dataset path: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/data/DG3/traj_data_for_model_2.pkl
model save path: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
log csv save path: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_log_08-24-13-54.csv
***********CHECK: len(self.trajectories) =5000
 ---- Visualizing input ----
CHECK: states.shape = torch.Size([100, 2])
CHECK: actions.shape = torch.Size([100, 1])
===== Note: rescaling states to original scale for viz=====
 ---- -------------- ----
**** initializing ContGridWorld_v5 environment *****
====================
init:  [19.5 20.5]
start_pos.shape=(2,)
xlim:  100
Umax=1.9990378869066041
Vmax=0.8324810714401253
Umean=0.41962545542847707
Vmean=0.2523411151246392
====================
act_dim = 1
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/core.py:329: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.
  deprecation(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.
  deprecation(
===== Note: rescaling states to original scale for viz=====
CHECK: states.shape = torch.Size([1, 120, 2])
CHECK: actions.shape = torch.Size([1, 120, 1])
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:190: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.
  logger.warn(f"{pre} is not within the observation space.")
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. 
  logger.deprecation(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: [33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: [33mWARN: The obs returned by the `step()` method is not within the observation space.
  logger.warn(f"{pre} is not within the observation space.")
actions;
 tensor([[[ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 0.9999],
         [ 0.9999],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 0.8673],
         [ 0.9999],
         [ 0.9999],
         [ 0.9999],
         [ 0.3779],
         [ 0.9998],
         [ 0.9999],
         [ 0.9999],
         [ 0.9913],
         [-0.9719],
         [ 0.8563],
         [-0.9742],
         [-0.6293],
         [ 0.9920],
         [ 1.0000],
         [-0.4996],
         [-0.6410],
         [ 0.9999],
         [ 0.9997],
         [-0.9749],
         [-0.9717],
         [ 0.9999],
         [-0.9639],
         [ 0.1820],
         [-0.9551],
         [ 0.9811],
         [-0.9220],
         [ 0.6707],
         [-0.4804],
         [-0.9452],
         [ 0.7472],
         [-0.5692],
         [ 0.9947],
         [-0.9668],
         [-0.6220],
         [-0.8662],
         [-0.9500],
         [ 0.7849],
         [-0.9755],
         [-0.2182],
         [-0.7048],
         [-0.9580],
         [-0.9551],
         [-0.9566],
         [-0.9641],
         [-0.9051],
         [-0.9373],
         [-0.9568],
         [-0.9187],
         [-0.9200],
         [-0.9727],
         [-0.9728],
         [-0.8260],
         [-0.9020],
         [-0.9080],
         [-0.9717],
         [ 0.9347],
         [-0.9084],
         [-0.9737],
         [ 0.9967],
         [-0.4981],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000]]])
============================================================
time elapsed: 0:00:09
num of updates: 100
action loss: 5.53937
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:00:14
num of updates: 200
action loss: 5.44675
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:00:19
num of updates: 300
action loss: 5.44672
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:00:25
num of updates: 400
action loss: 5.44670
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:00:30
num of updates: 500
action loss: 5.44669
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:00:35
num of updates: 600
action loss: 5.44669
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:00:40
num of updates: 700
action loss: 5.44668
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:00:45
num of updates: 800
action loss: 5.44668
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:00:50
num of updates: 900
action loss: 5.44668
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:00:55
num of updates: 1000
action loss: 5.44668
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:00
num of updates: 1100
action loss: 5.44668
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:05
num of updates: 1200
action loss: 5.44668
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:10
num of updates: 1300
action loss: 5.44668
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:15
num of updates: 1400
action loss: 5.44668
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:20
num of updates: 1500
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:26
num of updates: 1600
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:31
num of updates: 1700
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:36
num of updates: 1800
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:41
num of updates: 1900
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:46
num of updates: 2000
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
===== Note: rescaling states to original scale for viz=====
CHECK: states.shape = torch.Size([1, 120, 2])
CHECK: actions.shape = torch.Size([1, 120, 1])
actions;
 tensor([[[ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 0.9763],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 0.7709],
         [ 1.0000],
         [ 1.0000],
         [ 1.0000],
         [ 0.9990],
         [-0.9743],
         [ 0.9736],
         [-0.9800],
         [-0.3501],
         [ 0.9992],
         [ 1.0000],
         [-0.0573],
         [-0.3698],
         [ 1.0000],
         [ 1.0000],
         [-0.9797],
         [-0.9785],
         [ 1.0000],
         [-0.9646],
         [ 0.6702],
         [-0.9534],
         [ 0.9977],
         [-0.9067],
         [ 0.9073],
         [-0.2873],
         [-0.9376],
         [ 0.8168],
         [-0.2331],
         [ 0.9993],
         [-0.9703],
         [-0.3461],
         [-0.8127],
         [-0.9437],
         [ 0.9294],
         [-0.9788],
         [ 0.2718],
         [-0.5142],
         [-0.9577],
         [-0.9495],
         [-0.9531],
         [-0.9620],
         [-0.8730],
         [-0.9318],
         [-0.9568],
         [-0.8951],
         [-0.8964],
         [-0.9773],
         [-0.9768],
         [-0.7491],
         [-0.8649],
         [-0.8750],
         [-0.9734],
         [ 0.9865],
         [-0.8882],
         [-0.9781],
         [ 0.9993],
         [-0.2440],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000],
         [ 0.0000]]])
============================================================
time elapsed: 0:01:52
num of updates: 2100
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:01:57
num of updates: 2200
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:02
num of updates: 2300
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:07
num of updates: 2400
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:12
num of updates: 2500
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:17
num of updates: 2600
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:23
num of updates: 2700
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:28
num of updates: 2800
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:33
num of updates: 2900
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:38
num of updates: 3000
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:43
num of updates: 3100
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:48
num of updates: 3200
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:53
num of updates: 3300
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:02:58
num of updates: 3400
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:03:03
num of updates: 3500
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:03:08
num of updates: 3600
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:03:13
num of updates: 3700
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:03:18
num of updates: 3800
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
============================================================
time elapsed: 0:03:24
num of updates: 3900
action loss: 5.44667
eval avg reward: -191.00000
eval avg ep len: 92.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-24-13-54.pt
Traceback (most recent call last):
  File "/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/src/train_model.py", line 364, in <module>
    train('cfg', args, cfg_name, params2_name)
  File "/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/src/train_model.py", line 248, in train
    action_loss.backward()
  File "/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt