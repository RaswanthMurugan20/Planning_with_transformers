============================================================
start time: 08-25-16-20
============================================================
device set to: cuda
dataset path: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/data/DG3/traj_data_for_model_3.pkl
model save path: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
log csv save path: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_log_08-25-16-20.csv
***********CHECK: len(self.trajectories) =3150
 ---- Visualizing input ----
CHECK: states.shape = torch.Size([70, 2])
CHECK: actions.shape = torch.Size([70, 1])
===== Note: rescaling states to original scale for viz=====
 ---- -------------- ----
**** initializing ContGridWorld_v5 environment *****
====================
init:  [19.5 20.5]
start_pos.shape=(2,)
xlim:  100
Umax=1.9990378869066041
Vmax=0.8324810714401253
Umean=0.41962545542847707
Vmean=0.2523411151246392
====================
act_dim = 1
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/core.py:329: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.
  deprecation(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: [33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.
  deprecation(
[34m[1mwandb[39m[22m: logging graph, to disable use `wandb.watch(log_graph=False)`
===== Note: rescaling states to original scale for viz=====
CHECK: states.shape = torch.Size([1, 120, 2])
CHECK: actions.shape = torch.Size([1, 120, 1])
actions;
 tensor([[[0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.7992],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000]]])
============================================================
time elapsed: 0:00:07
num of updates: 100
action loss: 1.96308
eval avg reward: -177.00000
eval avg ep len: 78.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:190: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: [33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.
  logger.warn(f"{pre} is not within the observation space.")
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: [33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. 
  logger.deprecation(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: [33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64
  logger.warn(
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: [33mWARN: The obs returned by the `step()` method is not within the observation space.
  logger.warn(f"{pre} is not within the observation space.")
============================================================
time elapsed: 0:00:09
num of updates: 200
action loss: 1.94039
eval avg reward: -178.00000
eval avg ep len: 79.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:12
num of updates: 300
action loss: 1.91514
eval avg reward: -179.00000
eval avg ep len: 80.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:14
num of updates: 400
action loss: 1.90072
eval avg reward: -180.00000
eval avg ep len: 81.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:17
num of updates: 500
action loss: 1.87711
eval avg reward: -181.00000
eval avg ep len: 82.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:20
num of updates: 600
action loss: 1.84995
eval avg reward: -181.00000
eval avg ep len: 82.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:22
num of updates: 700
action loss: 1.84532
eval avg reward: -182.00000
eval avg ep len: 83.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:24
num of updates: 800
action loss: 1.82474
eval avg reward: -181.00000
eval avg ep len: 82.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:27
num of updates: 900
action loss: 1.82670
eval avg reward: -180.00000
eval avg ep len: 81.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:29
num of updates: 1000
action loss: 1.78567
eval avg reward: -180.00000
eval avg ep len: 81.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:32
num of updates: 1100
action loss: 1.66737
eval avg reward: -179.00000
eval avg ep len: 80.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:35
num of updates: 1200
action loss: 1.35946
eval avg reward: -176.00000
eval avg ep len: 77.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:37
num of updates: 1300
action loss: 1.32212
eval avg reward: -175.00000
eval avg ep len: 76.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:40
num of updates: 1400
action loss: 1.31509
eval avg reward: -174.00000
eval avg ep len: 75.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:42
num of updates: 1500
action loss: 1.29897
eval avg reward: -173.00000
eval avg ep len: 74.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:45
num of updates: 1600
action loss: 1.28536
eval avg reward: -173.00000
eval avg ep len: 74.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:47
num of updates: 1700
action loss: 1.28039
eval avg reward: -172.00000
eval avg ep len: 73.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:50
num of updates: 1800
action loss: 1.27082
eval avg reward: -172.00000
eval avg ep len: 73.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:52
num of updates: 1900
action loss: 1.25379
eval avg reward: -171.00000
eval avg ep len: 72.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:00:55
num of updates: 2000
action loss: 1.19186
eval avg reward: -171.00000
eval avg ep len: 72.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
===== Note: rescaling states to original scale for viz=====
CHECK: states.shape = torch.Size([1, 120, 2])
CHECK: actions.shape = torch.Size([1, 120, 1])
actions;
 tensor([[[0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.9688],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000],
         [0.0000]]])
============================================================
time elapsed: 0:00:57
num of updates: 2100
action loss: 1.15920
eval avg reward: -170.00000
eval avg ep len: 71.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:00
num of updates: 2200
action loss: 1.14837
eval avg reward: -170.00000
eval avg ep len: 71.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:02
num of updates: 2300
action loss: 1.14836
eval avg reward: -170.00000
eval avg ep len: 71.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:05
num of updates: 2400
action loss: 1.13728
eval avg reward: -170.00000
eval avg ep len: 71.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:07
num of updates: 2500
action loss: 1.12844
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:10
num of updates: 2600
action loss: 1.12751
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:12
num of updates: 2700
action loss: 1.12414
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:15
num of updates: 2800
action loss: 1.13092
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:17
num of updates: 2900
action loss: 1.11157
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:20
num of updates: 3000
action loss: 1.13149
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:22
num of updates: 3100
action loss: 1.11813
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:25
num of updates: 3200
action loss: 1.10820
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:27
num of updates: 3300
action loss: 1.11833
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:30
num of updates: 3400
action loss: 1.11701
eval avg reward: -169.00000
eval avg ep len: 70.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:32
num of updates: 3500
action loss: 1.10551
eval avg reward: -168.00000
eval avg ep len: 69.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:35
num of updates: 3600
action loss: 1.12168
eval avg reward: -168.00000
eval avg ep len: 69.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:37
num of updates: 3700
action loss: 1.11446
eval avg reward: -168.00000
eval avg ep len: 69.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:40
num of updates: 3800
action loss: 1.11534
eval avg reward: -168.00000
eval avg ep len: 69.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
time elapsed: 0:01:42
num of updates: 3900
action loss: 1.10972
eval avg reward: -168.00000
eval avg ep len: 69.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/src/model_min_dt.py:63: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  weights = torch.matmul(q, k.transpose(2,3)) / math.sqrt(D)
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/wandb/wandb_torch.py:285: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return tensor.shape == torch.Size([0]) or (~torch.isfinite(tensor)).all().item()
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/wandb/wandb_torch.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return tensor.shape == torch.Size([0]) or (~torch.isfinite(tensor)).all().item()
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/wandb/wandb_torch.py:288: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not torch.isfinite(tensor).all():
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/wandb/wandb_torch.py:203: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  tmin = flat.min().item()
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/wandb/wandb_torch.py:204: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  tmax = flat.max().item()
/home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/my-dt-venv/lib/python3.10/site-packages/wandb/wandb_torch.py:239: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  {name: wandb.Histogram(np_histogram=(tensor.tolist(), bins.tolist()))},
============================================================
time elapsed: 0:01:45
num of updates: 4000
action loss: 1.11766
eval avg reward: -168.00000
eval avg ep len: 69.00000
max score: -1.00000
saving current model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================
finished training!
============================================================
started training at: 08-25-16-20
finished training at: 22-08-25-16-22-03
total training time: 0:01:45
max d4rl score: -1.00000
saved max d4rl score model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20_best.pt
saved last updated model at: /home/rohit/Documents/Research/Planning_with_transformers/Decision_transformer/my-dec-transformer/log/my_dt_DG3_dummy_model_08-25-16-20.pt
============================================================